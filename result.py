import os
import tempfile
import hashlib
import streamlit as st
from dotenv import load_dotenv

from langchain.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain.chains import create_history_aware_retriever, create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_community.chat_message_histories.streamlit import StreamlitChatMessageHistory

# ğŸ” OpenAI API Key ì„¤ì •
load_dotenv()

# Streamlit UI êµ¬ì„±
st.set_page_config(page_title="íŒŒì¼ ì—…ë¡œë“œ + í—Œë²• Q&A ì±—ë´‡", layout="centered") #st.set_page_config() :ì•±ì˜ ì œëª©, ì•„ì´ì½˜, ë ˆì´ì•„ì›ƒ, ì´ˆê¸° ì‚¬ì´ë“œë°” ìƒíƒœ ë“±ì„ ì„¤ì •í•˜ëŠ” ë° ì‚¬ìš©. 
                                                                             #ë§¨ ì²˜ìŒì— í•œ ë²ˆë§Œ ì‚¬ìš©í•´ì•¼ í•¨.
st.header("ğŸ“„ ì—…ë¡œë“œëœ ë¬¸ì„œ ê¸°ë°˜ Q&A ì±—ë´‡ ğŸ’¬")

# GPT ëª¨ë¸ ì„ íƒ
selected_model = st.selectbox("ì‚¬ìš©í•  GPT ëª¨ë¸ì„ ì„ íƒí•˜ì„¸ìš”:", ("gpt-4o", "gpt-3.5-turbo-0125"))

# PDF ì—…ë¡œë“œ
uploaded_file = st.file_uploader("ğŸ“ PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”", type=["pdf"])

# ğŸ”‘ PDF í•´ì‹œ ìƒì„± í•¨ìˆ˜
def get_file_hash(file) -> str:   #ì—…ë¡œë“œí•œ íŒŒì¼ì„ ë°›ìŒ. ->strì€ stríƒ€ì…ì˜ ê°’ì„ ë°˜í™˜í•œë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸í•¨.(ê°œë°œìì˜ ì˜ë„ë¥¼ ì „ë‹¬í•˜ëŠ” ì˜ë„.)
    content = file.read()
    file.seek(0) #íŒŒì¼ í¬ì¸í„°ë¡œ file.read()ë¥¼ í•˜ì˜€ì„ ë•Œ, íŒŒì¼ í¬ì¸í„°ê°€ ëìœ¼ë¡œ ê°€ê²Œ ë˜ê¸°ë•Œë¬¸ì— ì´ë¥¼ ì²˜ìŒìœ¼ë¡œ ëŒë ¤ì£¼ê¸° ìœ„í•´ì„œ seekì„ ì‚¬ìš©.
                    #ì•ˆ í• ê²½ìš° read()ë¥¼ ë‹¤ì‹œ í•˜ì˜€ì„ ë•Œ ë¹ˆ ë¬¸ìì—´ ì¶œë ¥.
    return hashlib.md5(content).hexdigest()
    #íŒŒì¼ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ í•œ í•´ì‹œ ë¬¸ìì—´ ìƒì„±
    #í•´ì‹œë€, ë°ì´í„°ë¥¼ ê³ ì •ëœ ìš”ì•½ ê°’ìœ¼ë¡œ ë³€ê²½í•˜ëŠ” ê²ƒì„ ì´ì•¼ê¸°í•¨->ì•ˆì •ì„±,ë³´ì•ˆì„±(ì•ˆì „í•œ ë°ì´í„° ì „ì†¡)
    #md5ë€, 128ë¹„íŠ¸ì˜ ê³ ì •ëœ ê¸¸ì´ë¡œ ë°”ê¾¸ì–´ì¤€ë‹¤. ->ì¶©ëŒìœ„í—˜ì´ ìˆìŒ,í•˜ì§€ë§Œ ì—¬ê¸°ì„œëŠ” ì•ˆì „ì„ ìœ„í•´ì„œ ì“´ ê²ƒì´ ì•„ë‹ˆê¸° ë•Œë¬¸ì— ì¤‘ë³µì—¬ë¶€ì™€ ì‹ë³„ì„ ìœ„í•´ì„œ ì‚¬ìš©í•¨.ê·¸ë˜ì„œ ê´œì°®ë‹¤.
# âœ… PDF ë¡œë“œ ë° ë¶„í• 

@st.cache_resource  #íŒŒì¼ì„ ê³§ë°”ë¡œ pypdf_loaderì˜ ê²½ë¡œë¡œ ì…ë ¥í•  ìˆ˜ ì—†ìŒ. -> streamlitì€ íŒŒì¼ ê²½ë¡œê°€ ì•„ë‹Œ ìì²´ë¥¼ ì €ì¥í•˜ê¸° ë•Œë¬¸ì—
#ì„ì‹œ íŒŒì¼ì„ ë§Œë“¤ì–´ pdfì˜ ë‚´ìš©ì„ ê¸°ë¡í•˜ê³  ì´ íŒŒì¼ì˜ ê²½ë¡œë¥¼ ë³€ìˆ˜ì— ì €ì¥í•˜ëŠ” ì‹ì˜ ìš°íšŒì ì¸ ë°©ì‹ìœ¼ë¡œ ì ‘ê·¼í•´ì•¼ í•©ë‹ˆë‹¤.(p.239)
def load_and_split_pdf(file) -> list:
    with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp_file:
        tmp_file.write(file.read())
        tmp_file_path = tmp_file.name
    loader = PyPDFLoader(tmp_file_path)
    return loader.load_and_split()

# âœ… FAISS ì €ì¥/ë¡œë“œ í†µí•© í•¨ìˆ˜
@st.cache_resource  #ìºì‹œëœ ë‚´ìš©ì„ ì¬ì‚¬ìš©
def load_or_create_vectorstore(_docs, file_hash):  #ì„ë² ë”© ë²¡í„°dbê°€ ìˆìœ¼ë©´ ë¶ˆëŸ¬ì˜¤ê³  ì—†ìœ¼ë©´ ìƒì„±.
    index_path = os.path.join("faiss_index", file_hash) #ì €ì¥ëœ ë²¡í„° ì¸ë±ìŠ¤ê°€ ìœ„ì¹˜í•  ê²½ë¡œ.
    embedding_model = OpenAIEmbeddings(model="text-embedding-3-small") #í…ìŠ¤íŠ¸ ì„ë² ë”© ëª¨ë¸ ë¡œë”©.

    if os.path.exists(index_path):   #ë²¡í„° ì¸ë±ìŠ¤ê°€ ì´ë¯¸ ì¡´ì¬í•œë‹¤ë©´ë©´
        return FAISS.load_local(index_path, embedding_model) #ì €ì¥ëœ ë²¡í„° ì¸ë±ìŠ¤ ë¶ˆëŸ¬ì˜¤ê¸° ì¬ì‚¬ìš©ìœ¼ë¡œ ì¸í•´ ì‹œê°„ì ˆì•½(faissëŠ” ë¬´ì¡°ê±´ í•„ìš”)

    # ì—†ë‹¤ë©´ ìƒˆë¡œ ìƒì„±
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0) #ì„ë² ë”©ì„ ìœ„í•´ ìª¼ê°œì¤€ë‹¤.(ê²¹ì¹˜ëŠ” ë¶€ë¶„ ì—†ì´)
    split_docs = text_splitter.split_documents(_docs) #ìœ„ì—ì„œ ì •í•œ ê¸°ì¤€ìœ¼ë¡œ ë¶„í•  í›„ ë¦¬ìŠ¤íŠ¸ ìƒì„±
    for doc in split_docs:
        doc.metadata["source"] = f"{doc.metadata.get('source', 'ì—…ë¡œë“œ íŒŒì¼')} (p.{doc.metadata.get('page', 'n/a')})" #ë¬¸ì„œì˜ ì¶œì²˜ ì •ë³´ë¥¼ ëª…í™•íˆ í•˜ê¸° ìœ„í•´ metadata["source"] í•„ë“œë¥¼ ì—…ë°ì´íŠ¸
    vectorstore = FAISS.from_documents(split_docs, embedding_model) #ë¬¸ì„œ ì¡°ê°ë“¤ê³¼ ì„ë² ë”© ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ FAISS ë²¡í„° DBë¥¼ ìƒì„±(FAISSëŠ” ë¹ ë¥´ê³  íš¨ìœ¨ì ì¸ ë²¡í„° ê²€ìƒ‰ ë¼ì´ë¸ŒëŸ¬ë¦¬

    os.makedirs("faiss_index", exist_ok=True) #"faiss_index" í´ë”ê°€ ì—†ìœ¼ë©´ ìƒì„±(ìˆìœ¼ë©´ ì•„ë¬´ ì¼ë„ ì¼ì–´ë‚˜ì§€ ì•Šê²Œ exist_ok = True)
    vectorstore.save_local(index_path) #ìœ„ì—ì„œ ë§Œë“  ë²¡í„° ì¸ë±ìŠ¤ë¥¼ ë¡œì»¬ ê²½ë¡œì— ì €ì¥.(ë‹¤ìŒì„ ìœ„í•´ ìºì‹œ íŒŒì¼ë¡œ ìœ ì§€)
    return vectorstore#ë²¡í„° ì €ì¥ì†Œ(faiss)ë¶ˆëŸ¬ì˜¤ê¸°

# âœ… RAG ì²´ì¸ êµ¬ì„±
def initialize_rag_chain(docs, file_hash, selected_model):
    vectorstore = load_or_create_vectorstore(docs, file_hash)
    retriever = vectorstore.as_retriever()

    contextualize_q_prompt = ChatPromptTemplate.from_messages([
        ("system", "Given a chat history and a new question, return a standalone version of the question."),
        MessagesPlaceholder("history"),
        ("human", "{input}")
    ])

    qa_prompt = ChatPromptTemplate.from_messages([
        ("system", """You are an assistant for question-answering tasks. 
Use the following pieces of retrieved context to answer the question. 
If you don't know the answer, just say you don't know. 
Use polite Korean and include emoji.\n\n{context}"""),
        MessagesPlaceholder("history"),
        ("human", "{input}")
    ])

    llm = ChatOpenAI(model=selected_model)
    history_aware_retriever = create_history_aware_retriever(llm, retriever, contextualize_q_prompt)
    question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)
    rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)
    return rag_chain

# âœ… ëŒ€í™” ë©”ì‹œì§€ ê¸°ë¡ ì´ˆê¸°í™”
if "messages" not in st.session_state:
    st.session_state["messages"] = [{"role": "assistant", "content": "ì—…ë¡œë“œí•œ ë¬¸ì„œì— ëŒ€í•´ ê¶ê¸ˆí•œ ê²ƒì„ ì§ˆë¬¸í•´ ì£¼ì„¸ìš” ğŸ˜Š"}]

# âœ… íŒŒì¼ ì—…ë¡œë“œ í›„ ì‹¤í–‰
if uploaded_file:
    file_hash = get_file_hash(uploaded_file) #pdfë‚´ìš© ì½ì–´ì™€ì„œ md5í•´ì‹œê°’ìœ¼ë¡œ ê³„ì‚°.
    #íŒŒì¼ì‹ë³„í‚¤ì—­í• . ë²¡í„° ì¸ë±ìŠ¤ë¥¼ ì €ì¥í•˜ê±°ë‚˜ ë¶ˆëŸ¬ì˜¬ ë•Œ ê²½ë¡œ ì´ë¦„ìœ¼ë¡œ ì‚¬ìš©.
    with st.spinner("PDF ë¶„ì„ ì¤‘..."): #ìŠ¤í”¼ë„ˆuië¡œ ì‚¬ìš©ìì—ê²Œ ë¡œë”© ë©”ì‹œì§€í‘œì‹œ.
        pages = load_and_split_pdf(uploaded_file) #pdfì²˜ë¦¬í•¨ìˆ˜,ì„ì‹œ íŒŒì¼ ì €ì¥ ë° í˜ì´ì§€ í…ìŠ¤íŠ¸ë³€í™˜.
        #langchainì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” í˜•íƒœë¡œ ë³€í™˜ëœ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸.
        rag_chain = initialize_rag_chain(pages, file_hash, selected_model)
        #ë²¡í„°ìŠ¤í† ì–´ ë¶ˆëŸ¬ì˜¤ê±°ë‚˜ ë§Œë“¤ê³  ì„ íƒëœ llmê³¼ ì—°ê²°,ë¬¸ì„œ ê¸°ë°˜ ì§ˆë¬¸ì‘ë‹µ ì²´ì¸ìƒì„±.
        chat_history = StreamlitChatMessageHistory(key="chat_messages")
        conversational_chain = RunnableWithMessageHistory(
            rag_chain,
            lambda session_id: chat_history,
            input_messages_key="input",
            history_messages_key="history",
            output_messages_key="answer",
        )
        
    #ì‚¬ìš©ìê°€ PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ë©´,
    #íŒŒì¼ì˜ í•´ì‹œê°’ì„ ê³„ì‚°í•˜ê³ ,
    #PDFë¥¼ ë¶„ì„í•˜ê³  í˜ì´ì§€ë³„ í…ìŠ¤íŠ¸ë¡œ ë‚˜ëˆ•ë‹ˆë‹¤.
    #ê·¸ ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ RAG ì²´ì¸ì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.
    #ëŒ€í™” ì´ë ¥ì„ ê´€ë¦¬í•  ê°ì²´ë¥¼ ë§Œë“¤ê³ ,
    #ê·¸ê±¸ ì´ìš©í•´ ëŒ€í™”í˜• ì§ˆë¬¸ì‘ë‹µ ì²´ì¸ì„ êµ¬ì„±í•©ë‹ˆë‹¤.

    for msg in chat_history.messages:
        st.chat_message(msg.type).write(msg.content)

    if prompt := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”"):
        st.chat_message("human").write(prompt)
        with st.chat_message("ai"):
            with st.spinner("ë‹µë³€ ìƒì„± ì¤‘..."):
                config = {"configurable": {"session_id": "upload_session"}}
                response = conversational_chain.invoke({"input": prompt}, config)
                answer = response["answer"]
                st.write(answer)

                with st.expander("ğŸ” ì°¸ê³ í•œ ë¬¸ì„œ ë³´ê¸°"):
                    for doc in response.get("context", []):
                        st.markdown(f"ğŸ“„ {doc.metadata.get('source', 'ì•Œ ìˆ˜ ì—†ìŒ')}", help=doc.page_content)

#LangChainì˜ conversational_chainì„ í†µí•´ **PDF ê¸°ë°˜ ì§ˆë¬¸ ì‘ë‹µ(RAG)**ì„ ì‹¤í–‰í•˜ë©°,
#ê·¸ì— ë”°ë¥¸ ë‹µë³€ê³¼ ì°¸ê³  ë¬¸ì„œë¥¼ Streamlit UIì— ë³´ì—¬ì£¼ëŠ” ë¶€ë¶„
#ê³¼ê±° ëŒ€í™” ë‚´ìš©(chat history)ì„ í™”ë©´ì— ì¶œë ¥
#ì‚¬ìš©ìê°€ ì§ˆë¬¸ ì…ë ¥
#ì§ˆë¬¸ì„ AIì—ê²Œ ì „ë‹¬í•˜ê³ , ìŠ¤í”¼ë„ˆë¡œ ë¡œë”© í‘œì‹œ
#LangChain RAG ì²´ì¸ ì‹¤í–‰ â†’ ë‹µë³€ ìƒì„±
#ë‹µë³€ ì¶œë ¥ + ì°¸ê³ í•œ ë¬¸ì„œ ëª©ë¡ ì œê³µ (ì¶œì²˜ + ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°)
